{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Agents - Memory and Multiple Tools"
      ],
      "metadata": {
        "id": "zUUSv1qoEkf8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Sa0pI3uLyPxu"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Necessary Libraries\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from google.colab import userdata\n",
        "import os\n"
      ],
      "metadata": {
        "id": "-AIQZ1l86z-o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. API Key"
      ],
      "metadata": {
        "id": "Zw0lvzCKDDEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the API Key\n",
        "API_KEY = userdata.get('GOOGLE_API_KEY')\n"
      ],
      "metadata": {
        "id": "73I_WItD7puQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. LLM"
      ],
      "metadata": {
        "id": "F7bdicW4DM_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    google_api_key = API_KEY,\n",
        "    temperature = 0\n",
        ")\n"
      ],
      "metadata": {
        "id": "SR5STL3XDP-b"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Tools"
      ],
      "metadata": {
        "id": "zVTv0nzFDYPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making 2 tools\n",
        "# A function that takes a text (query) and returns text\n",
        "def calculator_tool(query : str) -> str:\n",
        "  try:\n",
        "    # eval() calculates the math inside query, then turn it into text\n",
        "    return str(eval(query))\n",
        "  except Exception as e:\n",
        "    return f\"Error: {e}\"\n",
        "\n",
        "def search_tool(query : str) -> str:\n",
        "  return f\"Search results for '{query}' (dummy response)\"\n"
      ],
      "metadata": {
        "id": "V19qY_fB75vu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naming both tools and adding functions and descriptions\n",
        "Tools = [\n",
        "    Tool(\n",
        "        name = \"Calculator\",\n",
        "        func = calculator_tool,\n",
        "        description = \"Useful for solving math expressions like '2+2' or '5*10'.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name = \"Search\",\n",
        "        func = search_tool,\n",
        "        description = \"Use to look up information about the world.\"\n",
        "    ),\n",
        "]\n"
      ],
      "metadata": {
        "id": "v7MqSgaH9cl1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Allocating Memory"
      ],
      "metadata": {
        "id": "VIQofRJcDdJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Allocating Memory\n",
        "memory =  ConversationBufferMemory(\n",
        "    memory_key = \"chat_history\",\n",
        "    return_messages = True\n",
        "    )\n"
      ],
      "metadata": {
        "id": "OYG8km1--vha"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Initializing Agent"
      ],
      "metadata": {
        "id": "JuKo4vmtDjQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing an Agent\n",
        "agent = initialize_agent(\n",
        "    tools = Tools,\n",
        "    memory = memory,\n",
        "    llm = llm,\n",
        "    agent = \"chat-conversational-react-description\",\n",
        "    verbose = False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgWDfyXkBc2w",
        "outputId": "403d80c8-178e-45af-b354-b69cafb723fc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-149431728.py:2: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Running Agents"
      ],
      "metadata": {
        "id": "dMHC4_pYDpzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the agents\n",
        "agent.run(\"Hi, my name is Noor!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F450v1TuB3Fz",
        "outputId": "58034604-873e-4f49-c9a8-417c39c9450b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi Noor, it's nice to meet you! I'm Assistant.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling memory\n",
        "agent.run(\"What is my name?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "34_t3jKwCI6P",
        "outputId": "60b775c5-392b-41b9-b781-1849ccda76c0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Noor.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the calculator tool\n",
        "agent.run(\"What is 344*65?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s4n-dx0XCSu0",
        "outputId": "aa501be4-7cf1-46cd-b0bb-393a1be1e021"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The answer to 344*65 is 22360.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the search tool\n",
        "agent.run(\"Search for LangChain basics(i.e. It's defination and a bit of explaination)!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "V5bYTS6XCgJJ",
        "outputId": "91f5d44f-0e3b-4f7d-fc64-24ef878aa98d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on my search, LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It provides tools and abstractions to chain together different components, such as prompts, models, and memory, to create more sophisticated applications. It can be used for various tasks like document question answering, chatbots, and more.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}