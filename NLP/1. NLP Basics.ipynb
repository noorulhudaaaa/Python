{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing - Basics\n",
        "Natural Language Processing (NLP) is a part of artificial intelligence that helps computers understand human language — like English.\n",
        "\n",
        "It lets computers:\n",
        "\n",
        "- Read what we write.\n",
        "- Hear what we say.\n",
        "- Understand the meaning.\n",
        "- Reply back like a human."
      ],
      "metadata": {
        "id": "l-XeZ65TiJh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Library\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37vsSEzGjnjM",
        "outputId": "e68a5b69-128a-4317-9b28-2b99467c9a86"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Necessary Libraries\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize, TreebankWordTokenizer\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import wordnet\n"
      ],
      "metadata": {
        "id": "U099LQ7aTP5E"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a corpus\n",
        "corpus = \"\"\"In Natural Language Processing (NLP), a corpus is a large collection of written or spoken texts.\n",
        "It is like a dataset made of language. Researchers and machines use a corpus to learn how people speak and write.\n",
        "For example, a corpus may contain books, articles, tweets, or even chat messages.\n",
        "By analyzing this data, these models can learn grammar, vocabulary, sentence structure, and meaning.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TQqD5upXTXha"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the corpus\n",
        "print(corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk9LahE7T8lI",
        "outputId": "fd6eade7-393a-4841-e691-4a9855f1c94d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Natural Language Processing (NLP), a corpus is a large collection of written or spoken texts. \n",
            "It is like a dataset made of language. Researchers and machines use a corpus to learn how people speak and write. \n",
            "For example, a corpus may contain books, articles, tweets, or even chat messages. \n",
            "By analyzing this data, these models can learn grammar, vocabulary, sentence structure, and meaning.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# punkt is a tool inside NLTK that knows how to split sentences and words properly.\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gov1jG59kXTX",
        "outputId": "10b5ec2c-fef5-4fa2-9cb7-277accae588b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tokenization\n",
        "Tokenization means breaking a sentence into smaller parts, like words or pieces.\n",
        "\n",
        "For example:\n",
        "Sentence: \"I love coding!\"\n",
        "After tokenization: [\"I\", \"love\", \"coding\", \"!\"]"
      ],
      "metadata": {
        "id": "xxWt5DGtUVe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1 - Sentence Tokenization\n",
        "Sentence tokenization means breaking a paragraph into sentences."
      ],
      "metadata": {
        "id": "A4MBmnzzY3nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing tokenization\n",
        "document = sent_tokenize(corpus)\n",
        "\n",
        "# Checking type\n",
        "type(document)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThQyoHxOUsJJ",
        "outputId": "1b3b2b42-75a9-4ba9-bba3-9ba6c71a1180"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking eah sentence after tokenization using loop\n",
        "for sentence in document:\n",
        "  print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XU-xzzMYU1N",
        "outputId": "0c3b6e40-da2c-4424-e879-67d23a2843bc"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Natural Language Processing (NLP), a corpus is a large collection of written or spoken texts.\n",
            "It is like a dataset made of language.\n",
            "Researchers and machines use a corpus to learn how people speak and write.\n",
            "For example, a corpus may contain books, articles, tweets, or even chat messages.\n",
            "By analyzing this data, these models can learn grammar, vocabulary, sentence structure, and meaning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2 - Word Tokenization\n",
        "Word tokenization means breaking a sentence into words."
      ],
      "metadata": {
        "id": "f5AITR_vZWXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing word tokenization\n",
        "words = word_tokenize(corpus)\n",
        "type(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdZNV91eZg1I",
        "outputId": "2190b76e-24f7-4cfe-99f0-0e21f5393137"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying each word in corpus\n",
        "for word in words:\n",
        "  print(word)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h_K4hosa0aV",
        "outputId": "7a5d5606-14a5-41c9-d552-da398d7a99b0"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In\n",
            "Natural\n",
            "Language\n",
            "Processing\n",
            "(\n",
            "NLP\n",
            ")\n",
            ",\n",
            "a\n",
            "corpus\n",
            "is\n",
            "a\n",
            "large\n",
            "collection\n",
            "of\n",
            "written\n",
            "or\n",
            "spoken\n",
            "texts\n",
            ".\n",
            "It\n",
            "is\n",
            "like\n",
            "a\n",
            "dataset\n",
            "made\n",
            "of\n",
            "language\n",
            ".\n",
            "Researchers\n",
            "and\n",
            "machines\n",
            "use\n",
            "a\n",
            "corpus\n",
            "to\n",
            "learn\n",
            "how\n",
            "people\n",
            "speak\n",
            "and\n",
            "write\n",
            ".\n",
            "For\n",
            "example\n",
            ",\n",
            "a\n",
            "corpus\n",
            "may\n",
            "contain\n",
            "books\n",
            ",\n",
            "articles\n",
            ",\n",
            "tweets\n",
            ",\n",
            "or\n",
            "even\n",
            "chat\n",
            "messages\n",
            ".\n",
            "By\n",
            "analyzing\n",
            "this\n",
            "data\n",
            ",\n",
            "these\n",
            "models\n",
            "can\n",
            "learn\n",
            "grammar\n",
            ",\n",
            "vocabulary\n",
            ",\n",
            "sentence\n",
            "structure\n",
            ",\n",
            "and\n",
            "meaning\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Can also be done by doing\n",
        "for sentence in document:\n",
        "  print(word_tokenize(sentence))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19v1fQPZbSag",
        "outputId": "295b8bfb-791d-4416-ee4f-5a4ddf2e4d55"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'a', 'corpus', 'is', 'a', 'large', 'collection', 'of', 'written', 'or', 'spoken', 'texts', '.']\n",
            "['It', 'is', 'like', 'a', 'dataset', 'made', 'of', 'language', '.']\n",
            "['Researchers', 'and', 'machines', 'use', 'a', 'corpus', 'to', 'learn', 'how', 'people', 'speak', 'and', 'write', '.']\n",
            "['For', 'example', ',', 'a', 'corpus', 'may', 'contain', 'books', ',', 'articles', ',', 'tweets', ',', 'or', 'even', 'chat', 'messages', '.']\n",
            "['By', 'analyzing', 'this', 'data', ',', 'these', 'models', 'can', 'learn', 'grammar', ',', 'vocabulary', ',', 'sentence', 'structure', ',', 'and', 'meaning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3 - Word Punctuation Tokenization\n",
        "- Just splits words based on punctuation.\n",
        "- Breaks \"don't\" into \"don\" and \"'\" and \"t\""
      ],
      "metadata": {
        "id": "wF1ZrQU2b7xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing the word punctuation tokenization\n",
        "wordPunctuation = wordpunct_tokenize(corpus)\n",
        "type(wordPunctuation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNB1IusHb64v",
        "outputId": "efc513a0-6b97-4ef9-fe5e-2c75507fe479"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying\n",
        "for word in wordPunctuation:\n",
        "  print(word)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLk7cTtkc-EE",
        "outputId": "6ce513ce-fa74-4405-c2dd-6b443f09bb55"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In\n",
            "Natural\n",
            "Language\n",
            "Processing\n",
            "(\n",
            "NLP\n",
            "),\n",
            "a\n",
            "corpus\n",
            "is\n",
            "a\n",
            "large\n",
            "collection\n",
            "of\n",
            "written\n",
            "or\n",
            "spoken\n",
            "texts\n",
            ".\n",
            "It\n",
            "is\n",
            "like\n",
            "a\n",
            "dataset\n",
            "made\n",
            "of\n",
            "language\n",
            ".\n",
            "Researchers\n",
            "and\n",
            "machines\n",
            "use\n",
            "a\n",
            "corpus\n",
            "to\n",
            "learn\n",
            "how\n",
            "people\n",
            "speak\n",
            "and\n",
            "write\n",
            ".\n",
            "For\n",
            "example\n",
            ",\n",
            "a\n",
            "corpus\n",
            "may\n",
            "contain\n",
            "books\n",
            ",\n",
            "articles\n",
            ",\n",
            "tweets\n",
            ",\n",
            "or\n",
            "even\n",
            "chat\n",
            "messages\n",
            ".\n",
            "By\n",
            "analyzing\n",
            "this\n",
            "data\n",
            ",\n",
            "these\n",
            "models\n",
            "can\n",
            "learn\n",
            "grammar\n",
            ",\n",
            "vocabulary\n",
            ",\n",
            "sentence\n",
            "structure\n",
            ",\n",
            "and\n",
            "meaning\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4 - Tree Bank Word Tokenizer\n",
        "TreebankWordTokenizer is a smart tokenizer that splits text into words using grammar rules, especially handling punctuation and contractions nicely."
      ],
      "metadata": {
        "id": "1I_1Q2v1dczx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using TreeBankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "treeB = tokenizer.tokenize(corpus)\n",
        "type(treeB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVHVz9eReE_t",
        "outputId": "0cd4e62a-28ce-46ba-8463-fd1200c1ace1"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying\n",
        "for word in treeB:\n",
        "  print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD7PinNFeUE7",
        "outputId": "f5e5bcf6-47d6-4e4c-876c-89b1c4fb7d12"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In\n",
            "Natural\n",
            "Language\n",
            "Processing\n",
            "(\n",
            "NLP\n",
            ")\n",
            ",\n",
            "a\n",
            "corpus\n",
            "is\n",
            "a\n",
            "large\n",
            "collection\n",
            "of\n",
            "written\n",
            "or\n",
            "spoken\n",
            "texts.\n",
            "It\n",
            "is\n",
            "like\n",
            "a\n",
            "dataset\n",
            "made\n",
            "of\n",
            "language.\n",
            "Researchers\n",
            "and\n",
            "machines\n",
            "use\n",
            "a\n",
            "corpus\n",
            "to\n",
            "learn\n",
            "how\n",
            "people\n",
            "speak\n",
            "and\n",
            "write.\n",
            "For\n",
            "example\n",
            ",\n",
            "a\n",
            "corpus\n",
            "may\n",
            "contain\n",
            "books\n",
            ",\n",
            "articles\n",
            ",\n",
            "tweets\n",
            ",\n",
            "or\n",
            "even\n",
            "chat\n",
            "messages.\n",
            "By\n",
            "analyzing\n",
            "this\n",
            "data\n",
            ",\n",
            "these\n",
            "models\n",
            "can\n",
            "learn\n",
            "grammar\n",
            ",\n",
            "vocabulary\n",
            ",\n",
            "sentence\n",
            "structure\n",
            ",\n",
            "and\n",
            "meaning\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming And Lemmatization\n",
        "- Stemming means cutting a word to its base/root form, even if the result is not a real word. It’s fast, but not always accurate.\n",
        "\n",
        "- Lemmatizing means turning a word into its real base word (called a lemma) using grammar rules. It’s slower but more accurate than stemming.\n"
      ],
      "metadata": {
        "id": "_cINH01Hen8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1- Stemming"
      ],
      "metadata": {
        "id": "zrpST3xskx8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a list\n",
        "words = [\n",
        "    \"running\",\n",
        "    \"flies\",\n",
        "    \"easily\",\n",
        "    \"studies\",\n",
        "    \"happily\",\n",
        "    \"better\",\n",
        "    \"played\",\n",
        "    \"playing\",\n",
        "    \"children\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "MSTDxApQfbSV"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing Stemming\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Applying for loop\n",
        "print(\"STEMMING:\\n\")\n",
        "for w in words:\n",
        "  print(w, \":\", stemmer.stem(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTRg4zwyhk7c",
        "outputId": "8cfba0c5-9ba2-4324-d611-302c7e255d15"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEMMING:\n",
            "\n",
            "running : run\n",
            "flies : fli\n",
            "easily : easili\n",
            "studies : studi\n",
            "happily : happili\n",
            "better : better\n",
            "played : play\n",
            "playing : play\n",
            "children : children\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2 - Lemmatization"
      ],
      "metadata": {
        "id": "dYTW948Jk2xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Using for loop for lemmatization\n",
        "print(\"Lemmatization:\\n\")\n",
        "for w in words:\n",
        "  print(w, \":\", lemmatizer.lemmatize(w))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6l19qvniLV9",
        "outputId": "6ac927a2-ce30-47f7-bfbe-f7a231f3c7f5"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization:\n",
            "\n",
            "running : running\n",
            "flies : fly\n",
            "easily : easily\n",
            "studies : study\n",
            "happily : happily\n",
            "better : better\n",
            "played : played\n",
            "playing : playing\n",
            "children : child\n"
          ]
        }
      ]
    }
  ]
}
